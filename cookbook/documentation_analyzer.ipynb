{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building a Documentation Analyzer with Hyperbrowser and GPT-4o\n",
                "\n",
                "In this cookbook, we'll create an intelligent documentation analyzer that can extract information from technical documentation and generate code based on that documentation. This agent will:\n",
                "\n",
                "1. Navigate to any documentation page or website\n",
                "2. Extract and understand the technical content\n",
                "3. Follow links to gather additional context when needed\n",
                "4. Generate working code examples based on documentation specifications\n",
                "5. Provide clear explanations and instructions for implementation\n",
                "\n",
                "This approach combines:\n",
                "- **[Hyperbrowser](https://hyperbrowser.ai)** for web scraping and documentation extraction\n",
                "- **OpenAI's GPT-4o** for technical understanding and code generation\n",
                "\n",
                "By the end of this cookbook, you'll have a versatile tool that can help developers quickly implement features from documentation without having to parse through lengthy technical material manually!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "Before starting, you'll need:\n",
                "\n",
                "1. A Hyperbrowser API key (sign up at [hyperbrowser.ai](https://hyperbrowser.ai) if you don't have one)\n",
                "2. An OpenAI API key with access to GPT-4o\n",
                "\n",
                "Store these API keys in a `.env` file in the same directory as this notebook:\n",
                "\n",
                "```\n",
                "HYPERBROWSER_API_KEY=your_hyperbrowser_key_here\n",
                "OPENAI_API_KEY=your_openai_key_here\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Set up imports and load environment variables\n",
                "\n",
                "We start by importing the necessary packages and initializing our environment variables. The key libraries we'll use include:\n",
                "- `asyncio` for handling asynchronous operations\n",
                "- `hyperbrowser` for web scraping and documentation extraction\n",
                "- `openai` for technical understanding and code generation\n",
                "- `IPython.display` for rendering markdown output in the notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 63,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 63,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import asyncio\n",
                "import json\n",
                "import os\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from hyperbrowser import AsyncHyperbrowser\n",
                "from hyperbrowser.tools import WebsiteScrapeTool\n",
                "from openai import AsyncOpenAI\n",
                "from openai.types.chat import (\n",
                "    ChatCompletionMessageParam,\n",
                "    ChatCompletionMessageToolCall,\n",
                "    ChatCompletionToolMessageParam,\n",
                ")\n",
                "from IPython.display import Markdown, display\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Initialize API clients\n",
                "\n",
                "Here we create instances of the Hyperbrowser and OpenAI clients using our API keys. These clients will be responsible for web scraping and code generation respectively."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 64,
            "metadata": {},
            "outputs": [],
            "source": [
                "hb = AsyncHyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "llm = AsyncOpenAI()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Implement the tool handler\n",
                "\n",
                "The tool handler function processes requests from the LLM to interact with our web scraping functionality. It:\n",
                "\n",
                "1. Receives tool call parameters from the LLM\n",
                "2. Validates that the requested tool is available\n",
                "3. Executes the web scraping operation with the specified parameters\n",
                "4. Returns the scraped content or handles any errors that occur\n",
                "\n",
                "This function is crucial for enabling the LLM to access documentation content dynamically as it explores technical specifications."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 65,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def handle_tool_call(\n",
                "    tc: ChatCompletionMessageToolCall,\n",
                ") -> ChatCompletionToolMessageParam:\n",
                "    print(f\"Handling tool call: {tc.function.name}\")\n",
                "\n",
                "    try:\n",
                "        if (\n",
                "            tc.function.name\n",
                "            != WebsiteScrapeTool.openai_tool_definition[\"function\"][\"name\"]\n",
                "        ):\n",
                "            raise ValueError(f\"Tool not found: {tc.function.name}\")\n",
                "\n",
                "        args = json.loads(tc.function.arguments)\n",
                "        print(args)\n",
                "        content = await WebsiteScrapeTool.async_runnable(hb=hb, params=args)\n",
                "\n",
                "        return {\"role\": \"tool\", \"tool_call_id\": tc.id, \"content\": content}\n",
                "\n",
                "    except Exception as e:\n",
                "        err_msg = f\"Error handling tool call: {e}\"\n",
                "        print(err_msg)\n",
                "        return {\n",
                "            \"role\": \"tool\",\n",
                "            \"tool_call_id\": tc.id,\n",
                "            \"content\": err_msg,\n",
                "            \"is_error\": True,  # type: ignore\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create the agent loop\n",
                "\n",
                "Now we implement the core agent loop that orchestrates the conversation between:\n",
                "1. The user (who asks for code implementation based on documentation)\n",
                "2. The LLM (which analyzes the request and determines what information is needed)\n",
                "3. Our tool (which fetches documentation content from websites)\n",
                "\n",
                "This recursive pattern allows for sophisticated interactions where the agent can gather information iteratively, exploring multiple documentation pages if necessary to fully understand the technical requirements before generating code."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 66,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def agent_loop(messages: list[ChatCompletionMessageParam]) -> str:\n",
                "    while True:\n",
                "        response = await llm.chat.completions.create(\n",
                "            messages=messages,\n",
                "            model=\"gpt-4o\",\n",
                "            tools=[\n",
                "                WebsiteScrapeTool.openai_tool_definition,\n",
                "            ],\n",
                "            max_completion_tokens=8000,\n",
                "        )\n",
                "\n",
                "        choice = response.choices[0]\n",
                "\n",
                "        # Append response to messages\n",
                "        messages.append(choice.message)  # type: ignore\n",
                "\n",
                "        # Handle tool calls\n",
                "        if (\n",
                "            choice.finish_reason == \"tool_calls\"\n",
                "            and choice.message.tool_calls is not None\n",
                "        ):\n",
                "            tool_result_messages = await asyncio.gather(\n",
                "                *[handle_tool_call(tc) for tc in choice.message.tool_calls]\n",
                "            )\n",
                "            messages.extend(tool_result_messages)\n",
                "\n",
                "        elif choice.finish_reason == \"stop\" and choice.message.content is not None:\n",
                "            return choice.message.content\n",
                "\n",
                "        else:\n",
                "            print(choice)\n",
                "            raise ValueError(f\"Unhandled finish reason: {choice.finish_reason}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Design the system prompt\n",
                "\n",
                "The system prompt is crucial for guiding the LLM's behavior. Our prompt establishes the agent as an expert coder analyzer that can:\n",
                "\n",
                "1. Extract technical information from documentation pages\n",
                "2. Follow links to gather additional context when needed\n",
                "3. Generate code based strictly on the documentation specifications\n",
                "4. Avoid making unsupported assumptions beyond what's in the documentation\n",
                "\n",
                "This approach ensures that the code generated is fully compliant with the documented API or library specifications."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 67,
            "metadata": {},
            "outputs": [],
            "source": [
                "SYSTEM_PROMPT = \"\"\"\n",
                "You are an expert coder analyzer. You have access to a 'scrape_webpage' tool which can be used to get markdown data from a webpage. You can analyze the webpage and generate code based on the information so scraped. Base whatever code you generate on the documentation you extract. Do not deviate from that or make your own assumptions.\n",
                "\n",
                "Keep in mind that some times, the information provided might not be sufficient, and you might have to scrape other pages to arrive at the appropriate documentation. Links to these pages can usually be obtained from the initial page itself.\n",
                "\n",
                "This is the link to a piece of documentation {link}. Analyze the documentation and generate code based on whatever the user requires you to do.\n",
                "\"\"\".strip()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Create a factory function for documentation analyzers\n",
                "\n",
                "Now we'll create a factory function that generates specialized documentation analyzer agents for any docs site. This function:\n",
                "\n",
                "1. Takes a URL to a documentation page as input\n",
                "2. Ensures the URL has the proper format (adding https:// if needed)\n",
                "3. Formats the system prompt with this URL\n",
                "4. Returns a function that can answer questions and generate code based on that documentation\n",
                "\n",
                "This approach makes our solution reusable for working with documentation from any library, API, or framework."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 68,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Coroutine, Any, Callable\n",
                "\n",
                "\n",
                "def make_documentation_analyzer(\n",
                "    link_to_docs: str,\n",
                ") -> Callable[..., Coroutine[Any, Any, str]]:\n",
                "    # Popular documentation providers like Gitbook, Mintlify etc automatically generate a llms.txt file\n",
                "    # for documentation sites hosted on their platforms.\n",
                "    if not (link_to_docs.startswith(\"http://\") or link_to_docs.startswith(\"https://\")):\n",
                "        link_to_docs = f\"https://{link_to_docs}\"\n",
                "\n",
                "    sysprompt = SYSTEM_PROMPT.format(\n",
                "        link=link_to_docs,\n",
                "    )\n",
                "\n",
                "    async def document_analyzer(question: str) -> str:\n",
                "        return await agent_loop(\n",
                "            [\n",
                "                {\"role\": \"system\", \"content\": sysprompt},\n",
                "                {\"role\": \"user\", \"content\": question},\n",
                "            ]\n",
                "        )\n",
                "\n",
                "    return document_analyzer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Test the documentation analyzer\n",
                "\n",
                "Let's test our agent by asking it to generate code based on the Hyperbrowser (our!) documentation. We'll request an example of how to get search results from Google, which will demonstrate the agent's ability to navigate documentation and generate practical code examples."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 69,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Handling tool call: scrape_webpage\n",
                        "{'url': 'https://docs.hyperbrowser.ai', 'scrape_options': {'include_tags': ['a'], 'exclude_tags': [], 'only_main_content': True}}\n",
                        "Handling tool call: scrape_webpage\n",
                        "{'url': 'https://docs.hyperbrowser.ai/get-started/quickstart/puppeteer', 'scrape_options': {'include_tags': ['p', 'code'], 'exclude_tags': [], 'only_main_content': True}}\n"
                    ]
                }
            ],
            "source": [
                "document_analyzer = make_documentation_analyzer(\"https://docs.hyperbrowser.ai\")\n",
                "doc_example = await document_analyzer(\n",
                "    \"Can you tell me how I could get the search result link and search result name from www.google.com?\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Display the generated code\n",
                "\n",
                "Now we'll display the results of our documentation analyzer. The agent has scraped the Hyperbrowser documentation, understood the relevant APIs for web scraping with Puppeteer, and generated a complete code example for extracting search results from Google."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 70,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/markdown": [
                            "To get the search result links and search result names from Google using Puppeteer with Hyperbrowser, you would need to set up an environment with Puppeteer and an API key from Hyperbrowser. Below is an example code in Node.js using Puppeteer:\n",
                            "\n",
                            "```javascript\n",
                            "import { connect } from \"puppeteer-core\";\n",
                            "import { config } from \"dotenv\";\n",
                            "config();\n",
                            "\n",
                            "const main = async () => {\n",
                            "  // Connect to the Hyperbrowser session\n",
                            "  const browser = await connect({\n",
                            "    browserWSEndpoint: `wss://connect.hyperbrowser.ai?apiKey=${process.env.HYPERBROWSER_API_KEY}`,\n",
                            "  });\n",
                            "\n",
                            "  const [page] = await browser.pages();\n",
                            "\n",
                            "  // Navigate to Google\n",
                            "  await page.goto(\"https://www.google.com\");\n",
                            "\n",
                            "  // Perform search\n",
                            "  await page.type('input[name=\"q\"]', \"example search query\");\n",
                            "  await page.keyboard.press('Enter');\n",
                            "  await page.waitForNavigation();\n",
                            "\n",
                            "  // Extract search results\n",
                            "  const searchResults = await page.evaluate(() => {\n",
                            "    const results = [];\n",
                            "    document.querySelectorAll('.tF2Cxc').forEach((result) => {\n",
                            "      const link = result.querySelector('a')?.href;\n",
                            "      const title = result.querySelector('h3')?.innerText;\n",
                            "      if (link && title) {\n",
                            "        results.push({ link, title });\n",
                            "      }\n",
                            "    });\n",
                            "    return results;\n",
                            "  });\n",
                            "\n",
                            "  console.log(searchResults);\n",
                            "\n",
                            "  // Clean up\n",
                            "  await browser.close();\n",
                            "};\n",
                            "\n",
                            "main();\n",
                            "```\n",
                            "\n",
                            "### Instructions:\n",
                            "\n",
                            "1. **Install Required Packages:**\n",
                            "   - Run `npm install puppeteer-core @hyperbrowser/sdk dotenv` to install the necessary packages.\n",
                            "\n",
                            "2. **Set Up Your Environment Variable:**\n",
                            "   - Obtain your API key from the Hyperbrowser dashboard and add it to a `.env` file as `HYPERBROWSER_API_KEY=your_api_key_here`.\n",
                            "\n",
                            "3. **Run the Script:**\n",
                            "   - Execute the script with Node.js, and it will print out the search results with links and titles.\n",
                            "\n",
                            "This will scrape the search result links and names from Google using Puppeteer with Hyperbrowser."
                        ],
                        "text/plain": [
                            "<IPython.core.display.Markdown object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "display(Markdown(doc_example))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Future Explorations\n",
                "\n",
                "There are many exciting ways to extend and enhance this documentation analyzer. Here are some directions for developers and users to explore:\n",
                "\n",
                "### Advanced Documentation Processing\n",
                "- **Multi-page Crawling**\n",
                "- **Schema and Type Inference**\n",
                "\n",
                "### Code Generation Capabilities\n",
                "- **Test Generation**\n",
                "- **Code Customization Options**\n",
                "\n",
                "### Documentation Analysis\n",
                "- **Documentation Quality Assessment**\n",
                "- **Missing Documentation Detection**\n",
                "\n",
                "This provides some exciting directions in which the documentation analyzer could develop further. It could become an even more powerful tool for developers, technical writers, and API designers, bridging the gap between documentation and implementation while improving the overall quality of both."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this cookbook, we built a powerful documentation analyzer using Hyperbrowser and GPT-4o. This agent can:\n",
                "\n",
                "1. Automatically extract technical information from documentation websites\n",
                "2. Navigate between related documentation pages to gather comprehensive context\n",
                "3. Generate working code examples based on the documented specifications\n",
                "4. Provide step-by-step instructions for implementation\n",
                "5. Ensure code is compliant with the official API patterns and best practices\n",
                "\n",
                "This pattern can be extended to create more sophisticated documentation tools, such as:\n",
                "- Multi-framework code generators that provide implementations in various languages\n",
                "- Integration assistants that combine multiple APIs according to documentation\n",
                "- Migration assistants that help convert code between different library versions\n",
                "- Documentation gap analyzers that identify missing or unclear sections"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Relevant Links\n",
                "- [Hyperbrowser](https://hyperbrowser.ai)\n",
                "- [Hyperbrowser Documentation](https://docs.hyperbrowser.ai)\n",
                "- [OpenAI API Documentation](https://platform.openai.com/docs/introduction)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
