{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building a YouTube Data Extraction Tool with Model Context Protocol and Hyperbrowser\n",
                "\n",
                "In this cookbook, we'll build a powerful YouTube data extraction server using the Model Context Protocol (MCP) and Hyperbrowser. This combination allows AI models to access YouTube data directly from your local machine through a standardized protocol.\n",
                "\n",
                "With this setup, you'll be able to:\n",
                "- Extract video descriptions from any YouTube video\n",
                "- Retrieve recent videos from any YouTube channel\n",
                "- Access community posts from YouTube creators\n",
                "\n",
                "The Model Context Protocol acts as a bridge between AI models and your local tools, enabling AI assistants to interact with web content they couldn't otherwise access. By the end, you'll have a robust server that can be extended for various YouTube data extraction needs!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "Before starting, you'll need:\n",
                "\n",
                "1. A Hyperbrowser API key (sign up at [hyperbrowser.ai](https://hyperbrowser.ai) if you don't have one)\n",
                "2. The MCP Python package (`pip install mcp`)\n",
                "3. Playwright for Python (`pip install playwright`)\n",
                "4. Pydantic for adding models when doing structured extraction (`pip install pydantic`)\n",
                "4. Python 3.9+ installed\n",
                "\n",
                "Store your API key in a `.env` file or set it as an environment variable as needed for the MCP client.\n",
                "\n",
                "So, for example for Claude, we'd modify the `claude_desktop_config.json` like so:\n",
                "\n",
                "```json\n",
                "{\n",
                "  \"mcpServers\": {\n",
                "    \"hyperbrowser-ytb\": {\n",
                "      \"command\": \"<PATH TO PYTHON>\",\n",
                "      \"args\": [\n",
                "        \"<PATH TO MAIN.PY>/main.py\"\n",
                "      ],\n",
                "      \"env\": {\n",
                "        \"HYPERBROWSER_API_KEY\": \"<HYPERBROWSER_API_KEY>\"\n",
                "      }\n",
                "    }\n",
                "  }\n",
                "}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Initialize the MCP Server\n",
                "\n",
                "We start by initializing a Fast MCP server. The Model Context Protocol (MCP) creates a standardized way for AI models to discover and interact with tools running on your local machine.\n",
                "\n",
                "Unlike traditional API integrations where the AI needs to be specifically trained on each endpoint, MCP allows AIs to dynamically discover available tools and their capabilities, making integration much more flexible and powerful."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "import asyncio\n",
                "import json\n",
                "import os\n",
                "\n",
                "from hyperbrowser import Hyperbrowser\n",
                "from hyperbrowser.models.scrape import StartScrapeJobParams, ScrapeOptions\n",
                "from hyperbrowser.models.extract import StartExtractJobParams\n",
                "from hyperbrowser.models.session import CreateSessionParams\n",
                "from playwright.async_api import async_playwright\n",
                "from typing import List\n",
                "\n",
                "from pydantic import BaseModel\n",
                "from mcp.server.fastmcp import FastMCP\n",
                "\n",
                "mcp = FastMCP(\"hyperbrowser-ytb\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Create a Video Description Extraction Tool\n",
                "\n",
                "Our first tool enables AI models to extract the description from any YouTube video. YouTube descriptions often contain valuable information including timestamps, links to related content, and detailed explanations.\n",
                "\n",
                "We use Hyperbrowser's targeted scraping capabilities to extract only the description element from the page, making the extraction both efficient and precise. The `include_tags` parameter focuses our scraping on just the description div."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_youtube_video_description(url: str) -> str:\n",
                "    \"\"\"Get the description of a youtube video from it's url\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    resp = hb.scrape.start_and_wait(\n",
                "        StartScrapeJobParams(\n",
                "            url=url,\n",
                "            scrape_options=ScrapeOptions(include_tags=[\"div#description\"]),\n",
                "            session_options=CreateSessionParams(use_proxy=True),\n",
                "        )\n",
                "    )\n",
                "    if resp.data:\n",
                "        return resp.data.model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(\n",
                "            \"Could not get video description: Youtube scrape result was None\"\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Build a Channel Video Listing Tool\n",
                "\n",
                "Next, we create a more complex tool that extracts recent videos from any YouTube channel. This tool goes beyond simple scraping by:\n",
                "\n",
                "1. Establishing a live browser session through Hyperbrowser\n",
                "2. Using Playwright to interact with the page's dynamic content\n",
                "3. Extracting structured data including video URLs, titles, and thumbnails\n",
                "\n",
                "This approach handles YouTube's JavaScript-heavy interface that would be difficult to scrape with traditional methods. The async implementation ensures efficient execution with proper waiting for content to load."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "async def get_youtube_channel_recent_videos(channel_url: str):\n",
                "    \"\"\"Get the url,title,and thumbnail for the recent videos of a youtube channel\"\"\"\n",
                "    channel_url = (\n",
                "        channel_url if channel_url.endswith(\"/videos\") else f\"{channel_url}/videos\"\n",
                "    )\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    session = hb.sessions.create(CreateSessionParams(use_proxy=True))\n",
                "    if session.ws_endpoint is None:\n",
                "        raise Exception(\"Could not make a hyperbrowser session\")\n",
                "    \n",
                "    async with async_playwright() as p:\n",
                "        browser = await p.chromium.connect_over_cdp(session.ws_endpoint)\n",
                "        page = await browser.new_page()\n",
                "        await page.goto(channel_url)\n",
                "        await asyncio.sleep(2.5)\n",
                "        grid_elements = await page.query_selector_all(\"ytd-rich-grid-media\")\n",
                "        video_infos = []\n",
                "        for grid_element in grid_elements:\n",
                "            video_info = {}\n",
                "            link_element = await grid_element.query_selector(\n",
                "                \"a.ytd-rich-grid-media.focus-on-expand\"\n",
                "            )\n",
                "            if link_element:\n",
                "                href = await link_element.get_attribute(\"href\")\n",
                "                title = await link_element.inner_text()\n",
                "                if href:\n",
                "                    video_info[\"url\"] = href\n",
                "                if title:\n",
                "                    video_info[\"title\"] = title\n",
                "            img_element = await grid_element.query_selector(\"img\")\n",
                "            if img_element:\n",
                "                img_src = await img_element.get_attribute(\"src\")\n",
                "                if img_src:\n",
                "                    video_info[\"thumbnail\"] = img_src\n",
                "            video_infos.append(json.dumps(video_info))\n",
                "\n",
                "        return video_infos\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create a Community Posts Extraction Tool\n",
                "\n",
                "Our final tool enables access to a channel's community posts - content that's typically not available through any official API. Community posts often contain announcements, polls, and updates that provide valuable context about a creator's work.\n",
                "\n",
                "We use Hyperbrowser's extract feature with Pydantic models to create a structured representation of the community posts. This combines the power of web automation with strong typing for reliable data extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CommunityPost(BaseModel):\n",
                "    text: str\n",
                "    comments: str\n",
                "    link: str\n",
                "\n",
                "\n",
                "class CommunityPostList(BaseModel):\n",
                "    posts: List[CommunityPost]\n",
                "\n",
                "\n",
                "@mcp.tool()\n",
                "def get_youtube_channel_recent_posts(channel_url: str) -> str:\n",
                "    \"\"\"Get the recent community posts of a youtube channel from it's url\"\"\"\n",
                "    channel_url = (\n",
                "        channel_url\n",
                "        if channel_url.endswith(\"/community\")\n",
                "        else f\"{channel_url}/community\"\n",
                "    )\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    session = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(\n",
                "            urls=[channel_url],\n",
                "            session_options=CreateSessionParams(use_proxy=True),\n",
                "            schema=CommunityPostList,\n",
                "        )\n",
                "    )\n",
                "\n",
                "    return session.model_dump_json()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Running the MCP Server\n",
                "\n",
                "Now we'll launch our MCP server to make our YouTube tools available to AI models. The MCP server uses stdio (standard input/output) as its transport mechanism, making it compatible with a wide range of clients, including Claude Desktop, Cline, Cursor, Windsurf among many more.\n",
                "\n",
                "When an AI model connects to this server, it will discover all three of our YouTube tools along with their documentation, parameter types, and return types - all through the standardized MCP protocol."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    # Initialize and run the server\n",
                "    mcp.run(transport=\"stdio\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this cookbook, we've built a powerful YouTube data extraction server using the Model Context Protocol and Hyperbrowser. This combination enables AI models to access YouTube content that would otherwise be unavailable to them.\n",
                "\n",
                "By leveraging MCP, we've created a standardized interface that allows any compatible AI to:\n",
                "- Extract video descriptions\n",
                "- List recent videos from channels\n",
                "- Access community posts\n",
                "\n",
                "All without requiring custom training or hardcoded integrations for each tool.\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "To take this further, you might consider:\n",
                "- Adding tools for video comments extraction\n",
                "- Implementing search functionality across YouTube\n",
                "- Creating tools for channel statistics and metrics\n",
                "- Building a user interface that works alongside the MCP server\n",
                "- Adding authentication to access private or members-only content\n",
                "\n",
                "The MCP protocol opens up possibilities far beyond YouTube - any web-based or local data source can be made available to AI models using this same pattern."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Relevant Links\n",
                "- [Hyperbrowser](https://hyperbrowser.ai)\n",
                "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
                "- [Playwright Documentation](https://playwright.dev/python/docs/intro)\n",
                "- [Pydantic Documentation](https://docs.pydantic.dev/latest/)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
