{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Real-Time News Extraction with Model Context Protocol\n",
                "\n",
                "In this cookbook, we'll build a powerful news extraction server using the Model Context Protocol (MCP) and Hyperbrowser. This integration enables AI models to access real-time news from various sources through your local machine, transforming how they can retrieve and analyze current events.\n",
                "\n",
                "With this setup, you'll be able to give AI models the ability to:\n",
                "\n",
                "- Search for latest news on any topic from Bing\n",
                "- Retrieve geographically relevant local news\n",
                "- Access structured news data with titles, summaries, and source information\n",
                "\n",
                "The Model Context Protocol serves as a standardized bridge between AI systems and local tools, enabling assistants to work with dynamic web content they couldn't otherwise access. Unlike traditional integrations that require hardcoded API endpoints, MCP enables dynamic discovery of available tools and their specifications. This allows AI models to automatically understand what tools are available, what parameters they need, and what outputs they provide - all without requiring manual integration work for each new tool.\n",
                "\n",
                "By the end of this cookbook, you'll have a robust news extraction system that dramatically expands what your AI can know about current events!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "Before starting, you'll need:\n",
                "\n",
                "1. A Hyperbrowser API key (sign up at hyperbrowser.ai if you don't have one)\n",
                "2. The MCP Python package (pip install mcp)\n",
                "3. Requests for Python (pip install requests)\n",
                "4. Pydantic for adding models when doing structured extraction (pip install pydantic)\n",
                "5. Python 3.9+ installed\n",
                "\n",
                "Store your API key in a .env file or set it as an environment variable as needed for the MCP client.\n",
                "\n",
                "So, for example for Claude, we'd modify the claude_desktop_config.json like so:\n",
                "\n",
                "```json\n",
                "{\n",
                "  \"mcpServers\": {\n",
                "    \"hyperbrowser-news\": {\n",
                "      \"command\": \"<PATH TO PYTHON>\",\n",
                "      \"args\": [\"<PATH TO MAIN.PY>/main.py\"],\n",
                "      \"env\": {\n",
                "        \"HYPERBROWSER_API_KEY\": \"<HYPERBROWSER_API_KEY>\"\n",
                "      }\n",
                "    }\n",
                "  }\n",
                "}\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Libraries and Set Up Environment\n",
                "\n",
                "We start by importing the necessary packages for our news extraction server. The key components include:\n",
                "\n",
                "- Hyperbrowser: For headless browser automation and structured data extraction\n",
                "- FastMCP: The Model Context Protocol server implementation\n",
                "- Pydantic: For creating strongly-typed data models\n",
                "- requests: For retrieving location data when providing local news\n",
                "\n",
                "Together, these libraries will enable us to create a robust news extraction system that can be discovered and used by AI assistants.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import requests\n",
                "\n",
                "from hyperbrowser import Hyperbrowser\n",
                "from hyperbrowser.models.extract import StartExtractJobParams\n",
                "from hyperbrowser.models.session import CreateSessionParams\n",
                "from typing import List, Optional\n",
                "\n",
                "from pydantic import BaseModel\n",
                "from mcp.server.fastmcp import FastMCP\n",
                "import urllib.parse"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Initialize the MCP Server\n",
                "\n",
                "Now we initialize our Model Context Protocol server with a meaningful identifier.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "mcp = FastMCP(\"hyperbrowser-news\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define Data Models for News Information\n",
                "\n",
                "Before implementing our extraction tools, we define structured data models using Pydantic. These models serve multiple important purposes:\n",
                "\n",
                "1. They provide a schema that Hyperbrowser uses for focused extraction\n",
                "2. They ensure consistent data structure that AI models can rely on\n",
                "3. They validate the extracted data to catch potential errors\n",
                "\n",
                "Our data models include NewsInfo for individual news items and NewsInfoList as a container for multiple items. Each news item captures key attributes like title, link, summary, and source - essential information for meaningful news analysis.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NewsInfo(BaseModel):\n",
                "    title: str\n",
                "    link: str\n",
                "    summary: str\n",
                "    source: str\n",
                "\n",
                "\n",
                "class NewsInfoList(BaseModel):\n",
                "    list: List[NewsInfo]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create the Bing News Search Tool\n",
                "\n",
                "Our first MCP tool provides access to Bing's news search functionality. This tool:\n",
                "\n",
                "1. Takes a topic query and optional page number parameter\n",
                "2. Constructs a properly formatted search URL using URL encoding\n",
                "3. Uses Hyperbrowser's extraction capabilities to gather structured news data\n",
                "4. Returns the results as a standardized JSON response\n",
                "\n",
                "The pagination support allows AI models to explore beyond the first page of results when needed, enabling more comprehensive news analysis. Hyperbrowser's extraction capabilities automatically structure the unorganized web data into our defined schema.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_bing_news(news_topic: str, page: Optional[int]) -> str:\n",
                "    \"\"\"Get the latest news articles from Bing for a specific topic\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    search_query = urllib.parse.quote_plus(news_topic)\n",
                "    search_url = f\"https://www.bing.com/news/search?q={search_query}&first={(max((page-1),0) if page is not None else 0)*10}\"\n",
                "    resp = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(urls=[search_url], schema=NewsInfoList)\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return NewsInfoList.model_validate(resp.data).model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(\"Could not get news from bing.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Create the Google News Search Tool\n",
                "\n",
                "Next, we implement a tool for accessing Google News. This provides an alternative source of information, which can be valuable for comparison or when seeking broader coverage."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_google_news(news_topic: str) -> str:\n",
                "    \"\"\"Get the latest news articles from Google News for a specific topic\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    search_query = urllib.parse.quote(news_topic)\n",
                "    search_url = f\"https://news.google.com/search?q={search_query}\"\n",
                "    resp = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(urls=[search_url], schema=NewsInfoList)\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return NewsInfoList.model_validate(resp.data).model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(\"Could not get news from google.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Create the Local News Tool\n",
                "\n",
                "Our most advanced tool enables AI models to retrieve geographically relevant news. This capability addresses a significant limitation of many AI systems - their inability to access location-specific information. The function works by:\n",
                "\n",
                "1. Either accepting explicit location parameters or detecting location from IP address\n",
                "2. Using multiple geolocation providers for reliable location detection\n",
                "3. Setting up a geographically appropriate proxy through Hyperbrowser\n",
                "4. Searching for local news through that proxy to ensure relevance\n",
                "\n",
                "This approach enables AI assistants to provide news that's actually relevant to a user's local area - a capability that dramatically enhances contextual awareness for location-specific queries.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_local_news(city: Optional[str], country: Optional[str]) -> str:\n",
                "    \"\"\"Get the local news for a location. If the location isn't provided, then it will be inferred from your IP.\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    if city is None or country is None:\n",
                "        resp = requests.get(\"https://ip.oxylabs.io/location\")\n",
                "        if resp.status_code == 200:\n",
                "            location_data = resp.json()\n",
                "            # Try to get city from different providers, falling back through the list\n",
                "            providers = [\"ip2location\", \"maxmind\", \"dbip\"]\n",
                "            for provider in providers:\n",
                "                if provider in location_data[\"providers\"]:\n",
                "                    city = location_data[\"providers\"][provider][\"city\"]\n",
                "                    country = location_data[\"providers\"][provider][\"country\"]\n",
                "                    break\n",
                "\n",
                "    if city is None or country is None:\n",
                "        raise ValueError(\"Could not get city or country info\")\n",
                "    else:\n",
                "        print(f\"City: {city}, Country: {country}\", file=sys.stderr)\n",
                "        search_query = urllib.parse.quote(\"local news\")\n",
                "        search_url = f\"https://www.bing.com/news/search?q={search_query}\"\n",
                "        resp = hb.extract.start_and_wait(\n",
                "            StartExtractJobParams(\n",
                "                urls=[search_url],\n",
                "                schema=NewsInfoList,\n",
                "                session_options=CreateSessionParams(\n",
                "                    use_proxy=True, proxy_city=city, proxy_country=country  # type: ignore\n",
                "                ),\n",
                "            )\n",
                "        )\n",
                "\n",
                "        if resp.data:\n",
                "            return NewsInfoList.model_validate(resp.data).model_dump_json()\n",
                "        else:\n",
                "            raise ValueError(\"Could not get news from google.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Running the MCP Server\n",
                "\n",
                "Now we'll launch our MCP server to make our news extraction tools available to AI models. The server uses stdio (standard input/output) as its transport mechanism, making it compatible with a wide range of AI integration platforms including Claude Desktop, Cline, Cursor, and Windsurf.\n",
                "\n",
                "When an AI model connects to this server, it will automatically discover all three of our news tools along with their documentation, parameter types, and return types - all through the standardized MCP protocol. This dynamic discovery is a key advantage of MCP over traditional hardcoded integrations.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "if __name__ == \"main\":\n",
                "    # Initialize and run the server\n",
                "    mcp.run(transport=\"stdio\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this cookbook, we've built a powerful real-time news extraction server using the Model Context Protocol and Hyperbrowser. This combination enables AI models to access current news content that would otherwise be unavailable to them due to training cutoffs or access limitations.\n",
                "\n",
                "By leveraging MCP, we've created a standardized interface that allows any compatible AI to:\n",
                "\n",
                "- Search for the latest news on any topic of interest\n",
                "- Access structured news data with titles, sources, and summaries\n",
                "- Retrieve geographically relevant local news\n",
                "\n",
                "All without requiring custom training or hardcoded integrations for each news source.\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "To take this news extraction system further, you might consider:\n",
                "\n",
                "- Creating specialized tools for financial, sports, or technology news with more indepth analysis\n",
                "- Adding historical news search capabilities\n",
                "- Implementing source credibility scoring\n",
                "\n",
                "The MCP protocol opens up possibilities far beyond news - any web-based or local data source can be made available to AI models using this same pattern, dramatically expanding their real-time knowledge capabilities.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Relevant Links\n",
                "\n",
                "- [Hyperbrowser](https://hyperbrowser.ai)\n",
                "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
                "- [Pydantic Documentation](https://docs.pydantic.dev/latest/)\n",
                "- [Claude Desktop](https://modelcontextprotocol.io/quickstart/user)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
