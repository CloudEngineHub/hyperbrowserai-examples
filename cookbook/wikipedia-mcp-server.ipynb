{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building a Wikipedia Knowledge Extraction Server with Model Context Protocol\n",
                "\n",
                "In this cookbook, we'll build a powerful Wikipedia knowledge extraction server using the Model Context Protocol (MCP) and Hyperbrowser. This integration enables AI models to access and extract structured information from Wikipedia through your local machine, dramatically expanding their reference capabilities.\n",
                "\n",
                "With this setup, you'll be able to give AI models the ability to:\n",
                "\n",
                "- Search Wikipedia for relevant articles on any topic\n",
                "- Extract complete article content with proper structure\n",
                "- Access article edit history to understand how information has evolved\n",
                "- Discover new articles through the random article exploration\n",
                "\n",
                "The Model Context Protocol creates a standardized bridge between AI systems and local tools, enabling assistants to work with dynamic web content they couldn't otherwise access. This approach allows your AI to break free from training cutoff limitations and work with the most current information available on Wikipedia.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "Before starting, you'll need:\n",
                "\n",
                "1. A Hyperbrowser API key (sign up at [hyperbrowser.ai](https://www.hyperbrowser.ai) if you don't have one)\n",
                "2. The MCP Python package (pip install mcp)\n",
                "3. Pydantic for structured data modeling (pip install pydantic)\n",
                "4. Python 3.9+ installed\n",
                "\n",
                "Store your API key in a .env file or set it as an environment variable as needed for the MCP client.\n",
                "\n",
                "For Claude Desktop users, you'll need to modify the claude_desktop_config.json like so:\n",
                "\n",
                "```json\n",
                "{\n",
                "  \"mcpServers\": {\n",
                "    \"hyperbrowser-wiki\": {\n",
                "      \"command\": \"<PATH TO PYTHON>\",\n",
                "      \"args\": [\"<PATH TO MAIN.PY>/main.py\"],\n",
                "      \"env\": {\n",
                "        \"HYPERBROWSER_API_KEY\": \"<HYPERBROWSER_API_KEY>\"\n",
                "      }\n",
                "    }\n",
                "  }\n",
                "}\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Import Libraries and Set Up Environment\n",
                "\n",
                "We start by importing the necessary packages for our Wikipedia extraction server. The key components include:\n",
                "\n",
                "- Hyperbrowser: For automated web extraction and parsing\n",
                "- FastMCP: The Model Context Protocol server implementation\n",
                "- Pydantic: For creating strongly-typed data models that structure our Wikipedia content\n",
                "- urllib.parse: For URL encoding article titles and search queries\n",
                "\n",
                "These libraries work together to create a robust Wikipedia knowledge system that AI models can discover and use through the MCP protocol.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import urllib.parse\n",
                "from typing import List, Literal, Optional\n",
                "\n",
                "from hyperbrowser import Hyperbrowser\n",
                "from hyperbrowser.models.extract import StartExtractJobParams\n",
                "from hyperbrowser.models.scrape import StartScrapeJobParams, ScrapeOptions\n",
                "from pydantic import BaseModel\n",
                "from mcp.server.fastmcp import FastMCP"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The requirements can be installed with - "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [],
            "source": [
                "pip install mcp hyperbrowser pydantic"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Initialize the MCP Server\n",
                "\n",
                "Now we initialize our Model Context Protocol server with a meaningful identifier. This identifier is what AI models will use to discover and connect to our Wikipedia tools.\n",
                "\n",
                "The MCP server is the bridge that exposes our Wikipedia extraction capabilities to AI models using a standardized interface. This standardization is what makes it possible for any MCP-compatible AI to discover and use our tools without custom training or integration work.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "mcp = FastMCP(\"hyperbrowser-wiki\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define Data Models for Wikipedia Content\n",
                "\n",
                "Before implementing our extraction tools, we need to define structured data models using Pydantic. These models are crucial for several reasons:\n",
                "\n",
                "1. They provide explicit schemas that guide Hyperbrowser's extraction process\n",
                "2. They ensure all Wikipedia data is consistently structured and validated\n",
                "3. They define clear interfaces that AI models can rely on when using our tools\n",
                "\n",
                "Our model hierarchy includes:\n",
                "\n",
                "- WikipediaArticle: For complete article content\n",
                "- WikipediaSearchResult: For individual search results\n",
                "- WikipediaSearchResultList: A collection of search results\n",
                "- WikipediaContent: A union type that can represent either search results or a complete article\n",
                "- WikipediaEdit: For tracking changes to articles\n",
                "- WikipediaEditHistory: A collection of edits for an article\n",
                "\n",
                "This rich type system enables precise, structured knowledge extraction from Wikipedia's complex content.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Pydantic models for Wikipedia data\n",
                "class WikipediaArticle(BaseModel):\n",
                "    title: str\n",
                "    summary: str\n",
                "    content: str\n",
                "    url: str\n",
                "\n",
                "\n",
                "class WikipediaSearchResult(BaseModel):\n",
                "    title: str\n",
                "    snippet: str\n",
                "    url: str\n",
                "\n",
                "\n",
                "class WikipediaSearchResultList(BaseModel):\n",
                "    results: List[WikipediaSearchResult]\n",
                "\n",
                "\n",
                "# Union model for Wikipedia search and article\n",
                "class WikipediaContent(BaseModel):\n",
                "    type: Literal[\"search\", \"article\"]  # \"search\" or \"article\"\n",
                "    search_results: Optional[List[WikipediaSearchResult]] = None\n",
                "    article: Optional[WikipediaArticle] = None\n",
                "\n",
                "\n",
                "class WikipediaEdit(BaseModel):\n",
                "    editor: str\n",
                "    timestamp: str\n",
                "    summary: str\n",
                "    size_change: Optional[int]\n",
                "\n",
                "\n",
                "class WikipediaEditHistory(BaseModel):\n",
                "    title: str\n",
                "    edits: List[WikipediaEdit]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create the Wikipedia Search Tool\n",
                "\n",
                "Our first MCP tool provides a powerful search interface to Wikipedia. This tool:\n",
                "\n",
                "1. Takes a search query and constructs a properly formatted Wikipedia search URL\n",
                "2. Handles two distinct scenarios automatically:\n",
                "\n",
                "- When the search returns multiple results (returning structured search results)\n",
                "- When the search matches a specific article (returning the complete article)\n",
                "\n",
                "3. Uses a custom system prompt to guide Hyperbrowser's extraction process\n",
                "\n",
                "This intelligent handling means the AI doesn't need to make separate calls for searching and retrieving articles - the tool automatically determines the appropriate behavior based on Wikipedia's response.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def search_wikipedia(query: str) -> str:\n",
                "    \"\"\"Search Wikipedia for articles matching the query\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    search_query = urllib.parse.quote_plus(query)\n",
                "    search_url = f\"https://en.wikipedia.org/w/index.php?search={search_query}\"\n",
                "\n",
                "    resp = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(\n",
                "            urls=[search_url],\n",
                "            schema=WikipediaContent,\n",
                "            system_prompt=\"\"\"Your task is to extract information from Wikipedia pages. There are two possible scenarios:\n",
                "\n",
                "    1. Search Results Page:\n",
                "    - Extract all search results including title, snippet, and URL\n",
                "    - Include only actual article results (ignore special pages, categories etc.)\n",
                "    - Limit to the first page of results\n",
                "\n",
                "    2. Direct Article Page (when search exactly matches an article title):\n",
                "    - Extract the full article content including title, introduction, sections, and references\n",
                "    - Do not extract any search results in this case\n",
                "    - Ensure proper handling of article redirects\n",
                "\n",
                "    Set the 'type' field to either \"search\" or \"article\" accordingly.\n",
                "    Return structured data matching the WikipediaContent schema.\"\"\",\n",
                "        )\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return WikipediaContent.model_validate(resp.data).model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(\"Could not get search results from Wikipedia.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Create the Raw Wikipedia Content Tool\n",
                "\n",
                "Next, we implement a tool for extracting raw content from Wikipedia articles. Unlike our search tool, this function:\n",
                "\n",
                "1. Takes a specific article title rather than a search query\n",
                "2. Uses Hyperbrowser's scrape functionality (rather than extract) to obtain the complete, unstructured content\n",
                "3. Returns the content in multiple formats, including Markdown and with all links preserved\n",
                "\n",
                "This approach gives AI models access to the complete, unprocessed Wikipedia article content when they need the full context rather than a structured extraction.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_raw_wikipedia(title: str) -> str:\n",
                "    \"\"\"Get the raw content of a Wikipedia article by title\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "\n",
                "    formatted_title = urllib.parse.quote(title.replace(\" \", \"_\"))\n",
                "    search_url = f\"https://en.wikipedia.org/wiki/{formatted_title}\"\n",
                "\n",
                "    resp = hb.scrape.start_and_wait(\n",
                "        StartScrapeJobParams(\n",
                "            url=search_url, scrape_options=ScrapeOptions(formats=[\"markdown\", \"links\"])\n",
                "        )\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return json.dumps(resp.data.model_dump_json())\n",
                "    else:\n",
                "        raise ValueError(\"Could not get search results from Wikipedia.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Create the Wikipedia Article Tool\n",
                "\n",
                "This tool provides a more structured approach to retrieving complete Wikipedia articles. It differs from the raw content tool by:\n",
                "\n",
                "1. Using Hyperbrowser's extract functionality to create a structured representation\n",
                "2. Conforming to our WikipediaArticle model with clearly defined fields\n",
                "3. Providing a cleaner, more organized representation of the article's content\n",
                "\n",
                "This structured approach makes it easier for AI models to reason about and reference specific parts of Wikipedia articles, while maintaining the full informational content.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_wikipedia_article(title: str) -> str:\n",
                "    \"\"\"Get the full content of a Wikipedia article by title\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    formatted_title = urllib.parse.quote(title.replace(\" \", \"_\"))\n",
                "    article_url = f\"https://en.wikipedia.org/wiki/{formatted_title}\"\n",
                "\n",
                "    resp = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(urls=[article_url], schema=WikipediaArticle)\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return WikipediaArticle.model_validate(resp.data).model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(f\"Could not get article '{title}' from Wikipedia.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Create the Edit History Tool\n",
                "\n",
                "Understanding how Wikipedia content evolves is crucial for assessing its reliability. This tool enables AI models to access an article's edit history by:\n",
                "\n",
                "1. Converting the article title to a properly formatted history URL\n",
                "2. Using a specialized system prompt to guide the extraction of edit information\n",
                "3. Structuring the edit history according to our WikipediaEditHistory model\n",
                "\n",
                "This capability allows AI models to assess how recently an article has been updated, identify controversial sections (those with frequent edits), and understand the evolution of knowledge on a topic over time.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_wikipedia_edit_summary(title: str) -> str:\n",
                "    \"\"\"Get the edit history of a Wikipedia article by title\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    formatted_title = urllib.parse.quote(title.replace(\" \", \"_\"))\n",
                "    history_url = (\n",
                "        f\"https://en.wikipedia.org/w/index.php?title={formatted_title}&action=history\"\n",
                "    )\n",
                "\n",
                "    resp = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(\n",
                "            urls=[history_url],\n",
                "            schema=WikipediaEditHistory,\n",
                "            system_prompt=\"\"\"Extract the edit history from this Wikipedia page. Include:\n",
                "    - The timestamp of each edit\n",
                "    - The editor's username or IP\n",
                "    - The edit summary/comment\n",
                "    - The size change (+/- bytes)\n",
                "    \"\"\",\n",
                "        )\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return WikipediaEditHistory.model_validate(resp.data).model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(\n",
                "            f\"Could not get edit history for article '{title}' from Wikipedia.\"\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Create the Random Article Tool\n",
                "\n",
                "Serendipitous discovery is a powerful way to expand knowledge. This tool enables AI models to explore Wikipedia randomly by:\n",
                "\n",
                "1. Accessing Wikipedia's Special:Random page which redirects to a random article\n",
                "2. Extracting the complete content of whatever article is returned\n",
                "3. Structuring it according to our WikipediaArticle model\n",
                "\n",
                "This capability can be particularly valuable for exploration tasks, generating diverse examples, or simply expanding an AI's knowledge in unexpected directions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@mcp.tool()\n",
                "def get_random_article() -> str:\n",
                "    \"\"\"Get a random Wikipedia article\"\"\"\n",
                "    hb = Hyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))\n",
                "    random_url = \"https://en.wikipedia.org/wiki/Special:Random\"\n",
                "\n",
                "    resp = hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(urls=[random_url], schema=WikipediaArticle)\n",
                "    )\n",
                "\n",
                "    if resp.data:\n",
                "        return WikipediaArticle.model_validate(resp.data).model_dump_json()\n",
                "    else:\n",
                "        raise ValueError(\"Could not get a random article from Wikipedia.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Running the MCP Server\n",
                "\n",
                "Finally, we'll launch our MCP server to make our Wikipedia extraction tools available to AI models. The server uses stdio (standard input/output) as its transport mechanism, making it compatible with a wide range of AI clients including Claude Desktop, Cline, Cursor, and other MCP-compatible systems.\n",
                "\n",
                "When an AI model connects to this server, it will automatically discover all five of our Wikipedia tools along with their documentation, parameter types, and return types - all through the standardized MCP protocol.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "if __name__ == \"main\":\n",
                "    # Initialize and run the server\n",
                "    mcp.run(transport=\"stdio\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this cookbook, we've built a powerful Wikipedia knowledge extraction system using the Model Context Protocol and Hyperbrowser. This combination enables AI models to access Wikipedia's vast repository of information in ways that would otherwise be impossible due to training cutoffs or API limitations.\n",
                "\n",
                "By leveraging MCP, we've created a standardized interface that allows any compatible AI to:\n",
                "\n",
                "- Search Wikipedia and retrieve relevant articles\n",
                "- Extract complete article content in both structured and raw formats\n",
                "- Access article edit histories to evaluate information credibility\n",
                "- Explore new topics through random article discovery\n",
                "\n",
                "All without requiring custom training or hardcoded integrations for each specific task.\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "To take this Wikipedia extraction system further, you might consider:\n",
                "\n",
                "- Adding support for multiple languages beyond English Wikipedia\n",
                "- Implementing category browsing functionality\n",
                "- Creating tools for extracting structured data from infoboxes\n",
                "\n",
                "The MCP protocol opens up possibilities far beyond Wikipedia - any web-based or local data source can be made available to AI models using this same pattern, dramatically expanding their knowledge capabilities.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Relevant Links\n",
                "- [Hyperbrowser](https://hyperbrowser.ai)\n",
                "- [Hyperbrowser documentation](https://docs.hyperbrowser.ai)\n",
                "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
                "- [Claude Desktop](https://modelcontextprotocol.io/quickstart/user)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
