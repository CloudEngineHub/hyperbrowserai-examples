{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building an AI Company Research Agent with Hyperbrowser and GPT-4o\n",
                "\n",
                "In this cookbook, we'll build a Company Research Agent that can generate detailed reports on any company for any topic by automatically searching the web, analyzing search results, and compiling information from relevant sources.\n",
                "\n",
                "We'll use these tools to build our agent:\n",
                "- **[Hyperbrowser](https://hyperbrowser.ai)** for web search and reading web pages\n",
                "- **OpenAI's GPT-4o** for intelligent analysis and report generation\n",
                "\n",
                "By the end of this cookbook, you'll have a reusable agent that can research any company based on specific objectives, saving hours of manual research work!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "To follow along you'll need the following:\n",
                "\n",
                "1. A Hyperbrowser API key (sign up at [hyperbrowser.ai](https://hyperbrowser.ai) if you don't have one, it's free)\n",
                "2. An OpenAI API key (sign up at [openai.com](https://openai.com) if you don't have one, it's free)\n",
                "\n",
                "Both API keys should be stored in a `.env` file in the same directory as this notebook with the following format:\n",
                "\n",
                "```shell\n",
                "HYPERBROWSER_API_KEY=your_hyperbrowser_key_here\n",
                "OPENAI_API_KEY=your_openai_key_here\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Set up imports and load environment variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "from urllib.parse import urlencode\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from hyperbrowser import AsyncHyperbrowser\n",
                "from hyperbrowser.models.extract import StartExtractJobParams\n",
                "from hyperbrowser.models.scrape import ScrapeOptions, StartBatchScrapeJobParams\n",
                "from hyperbrowser.models.session import CreateSessionParams\n",
                "from openai import AsyncOpenAI\n",
                "from pydantic import BaseModel\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Initialize clients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "oai = AsyncOpenAI()\n",
                "hb = AsyncHyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define search functionality\n",
                "\n",
                "Now we'll create the models and function to search for information about a company. This function:\n",
                "\n",
                "1. Constructs a search URL with the company name and research objective\n",
                "2. Uses Hyperbrowser's extract feature to get structured data from search results\n",
                "3. Returns the search results in a structured format using Pydantic models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SearchResult(BaseModel):\n",
                "    \"\"\"A search result from Bing\"\"\"\n",
                "\n",
                "    title: str\n",
                "    url: str\n",
                "    content: str\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Title: {self.title}\\nURL: {self.url}\\nContent: {self.content}\"\n",
                "\n",
                "\n",
                "class SearchResults(BaseModel):\n",
                "    \"\"\"A list of search results from Bing\"\"\"\n",
                "\n",
                "    results: list[SearchResult]\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"\\n\\n{'-' * 10}\\n\\n\".join(str(result) for result in self.results)\n",
                "\n",
                "\n",
                "async def search_company(company_name: str, objective: str) -> SearchResults | None:\n",
                "    params = urlencode({\"q\": f\"{company_name} {objective}\"})\n",
                "    url = f\"https://www.bing.com/search?{params}\"\n",
                "\n",
                "    print(url)\n",
                "\n",
                "    result = await hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(\n",
                "            urls=[url],\n",
                "            prompt=\"Extract the title, url, and content of the top 10 search results on this page.\",\n",
                "            schema=SearchResults,\n",
                "        )\n",
                "    )\n",
                "\n",
                "    if not (result.status == \"completed\" and result.data):\n",
                "        raise Exception(\"Failed to extract search results\")\n",
                "\n",
                "    return SearchResults.model_validate(result.data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Choose relevant URLs with GPT-4o\n",
                "\n",
                "After getting search results, we need to intelligently select the most relevant URLs to scrape. This function:\n",
                "\n",
                "1. Uses GPT-4o to analyze the search results and choose the most relevant URLs\n",
                "2. Applies a system prompt that guides the model to think step-by-step\n",
                "3. Returns both the model's reasoning (chain of thought) and the selected URLs\n",
                "\n",
                "You can also use a different re-ranker model here but for simplicity we've used GPT-4o."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RelevantUrls(BaseModel):\n",
                "    chain_of_thought: str\n",
                "    urls: list[str]\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Chain of Thought: {self.chain_of_thought}\\n\\nURLs:\\n{self.urls}\"\n",
                "\n",
                "\n",
                "RELEVANT_URLS_SYSTEM_PROMPT = \"\"\"\n",
                "You are helping analyze search results to find the most relevant URLs for researching a company. \\\n",
                "Think step by step about which of the URLs you are given would be most useful considering the \\\n",
                "objective and the company name. Always respond with both your chain of thought and the list of \\\n",
                "relevant URLs.\"\"\".strip()\n",
                "\n",
                "\n",
                "async def choose_relevant_urls(company_name: str, objective: str, search_results: SearchResults) -> RelevantUrls:\n",
                "    response = await oai.beta.chat.completions.parse(\n",
                "        model=\"gpt-4o\",\n",
                "        messages=[\n",
                "            { \"role\": \"system\", \"content\": RELEVANT_URLS_SYSTEM_PROMPT },\n",
                "            {\n",
                "                \"role\": \"user\", \n",
                "                \"content\": (\n",
                "                    f\"Company: {company_name}\\nResearch Objective: {objective}\"\n",
                "                    f\"\\n\\nSearch Results:\\n{search_results}\"\n",
                "\n",
                "                    \"Think through which URLs would be most relevant and explain your reasoning. Return your analysis and final list of URLs along with your chain of thought.\"\n",
                "                )\n",
                "            }\n",
                "        ],\n",
                "        response_format=RelevantUrls\n",
                "    )\n",
                "\n",
                "    return response.choices[0].message.parsed"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Scrape content from selected URLs\n",
                "\n",
                "Once we have the most relevant URLs, we need to scrape their content. This function:\n",
                "\n",
                "1. Uses Hyperbrowser's batch scraping capability to process multiple URLs efficiently\n",
                "2. Configures advanced scraping options like proxy usage, stealth mode, and CAPTCHA solving\n",
                "3. Returns the scraped content in a structured format for further processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def scrape_urls(urls: list[str]) -> str:\n",
                "    scrape_result = await hb.scrape.batch.start_and_wait(\n",
                "        StartBatchScrapeJobParams(\n",
                "            urls=urls,\n",
                "            scrape_options=ScrapeOptions(formats=[\"markdown\"], only_main_content=True),\n",
                "            session_options=CreateSessionParams(\n",
                "                use_proxy=True,\n",
                "                use_stealth=True,\n",
                "                adblock=True,\n",
                "                trackers=True,\n",
                "                annoyances=True,\n",
                "                solve_captchas=True,\n",
                "            )\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    if scrape_result.status != \"completed\" or not scrape_result.data:\n",
                "        raise Exception(\"Failed to scrape URLs\")\n",
                "    \n",
                "    return \"\\n\".join([\n",
                "        f\"<website>\\n<url>{page.url}</url>\\n<content>\\n{page.markdown}\\n</content>\\n</website>\"\n",
                "        for page in scrape_result.data\n",
                "    ])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Generate research report with GPT-4o\n",
                "\n",
                "With the scraped content in hand, we can now generate a comprehensive research report. This function:\n",
                "\n",
                "1. Uses GPT-4o with a specialized system prompt for research analysis\n",
                "2. Provides the model with the company name, research objective, and scraped data\n",
                "3. Returns both the model's reasoning process and the final formatted report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ResearchAnalysis(BaseModel):\n",
                "    chain_of_thought: str\n",
                "    report: str\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Chain of Thought:\\n{self.chain_of_thought}\\n\\nReport:\\n{self.report}\"\n",
                "\n",
                "\n",
                "\n",
                "FINAL_REPORT_SYSTEM_PROMPT = \"\"\"\n",
                "You are an expert research analyst that compiles information from a list of websites into a report about a company.\n",
                "\n",
                "You will be given the following information:\n",
                "- Company Name\n",
                "- Research Objective\n",
                "- Scraped Data from the most relevant webpages about the company as it relates to the research objective / topic\n",
                "\n",
                "Your job is to compile the information into a report that is easy to understand and can be used to make decisions about the company.\n",
                "\n",
                "The report should be:\n",
                "- In markdown format\n",
                "- Concise and to the point\n",
                "- Include the company name, objective, and the information found on the websites\n",
                "- Be easy to understand and can be used to make decisions about the company\n",
                "\n",
                "Additionally, you should also maintain a scratchpad for your own notes and thoughts about the company as you draft the report.\n",
                "\n",
                "You must respond with both your chain of thought and the final report.\"\"\".strip()\n",
                "\n",
                "async def compile_into_report(scraped_data: str, company_name: str, objective: str) -> ResearchAnalysis:\n",
                "    response = await oai.beta.chat.completions.parse(\n",
                "        model=\"gpt-4o\",\n",
                "        messages=[\n",
                "            { \n",
                "                \"role\": \"system\",\n",
                "                \"content\": FINAL_REPORT_SYSTEM_PROMPT,\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\", \n",
                "                \"content\": (\n",
                "                    f\"Company Name: {company_name}\\nResearch Objective: {objective}\"\n",
                "                    f\"\\nScraped Websites / Articles: {scraped_data}\"\n",
                "                ),\n",
                "            }\n",
                "        ],\n",
                "        response_format=ResearchAnalysis\n",
                "    )\n",
                "\n",
                "    analysis = response.choices[0].message.parsed\n",
                "\n",
                "    return analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Combine all steps into a single research pipeline\n",
                "\n",
                "Now we'll create a single function that orchestrates the entire research process from start to finish. This function:\n",
                "\n",
                "1. Searches for information about the company\n",
                "2. Selects the most relevant URLs\n",
                "3. Scrapes content from those URLs\n",
                "4. Generates a comprehensive research report\n",
                "5. Returns the final analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def research_company(company_name: str, objective: str) -> ResearchAnalysis:\n",
                "    search_results = await search_company(company_name, objective)\n",
                "    relevant_urls = await choose_relevant_urls(company_name, objective, search_results)\n",
                "    scraped_data = await scrape_urls(relevant_urls.urls)\n",
                "    analysis = await compile_into_report(scraped_data, company_name, objective)\n",
                "    return analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Test the research agent\n",
                "\n",
                "Let's test our agent by researching UiPath's competitive landscape. This will demonstrate the full workflow:\n",
                "\n",
                "1. The agent searches for information about UiPath's competition\n",
                "2. It selects the most relevant URLs from the search results\n",
                "3. It scrapes content from those URLs\n",
                "4. It generates a comprehensive research report\n",
                "\n",
                "You'll see the search URL being printed, followed by the final analysis that includes both the reasoning process and the formatted report."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "https://www.bing.com/search?q=UiPath+Competition\n",
                        "Chain of Thought:\n",
                        "1. **Identifying Information about UiPath and its Position in the Market:**\\n   - UiPath is a leading player in the Robotic Process Automation (RPA) industry, focusing on providing automation software.\\n   - Founded in 2005, it has expanded its capabilities to include AI and machine learning.\\n   - UiPath's platform is known for being user-friendly and scalable, designed to automate repetitive tasks efficiently.\\n   - The company is headquartered in New York City and operated worldwide.\\n\\n2. **Key Competitors of UiPath:**\\n   - **Automation Anywhere**: Founded in 2003 and headquartered in San Jose, California, known for its cloud-native platform and robust RPA tools. It emphasizes ease of use and AI integrations.\\n   - **Blue Prism**: Established in 2001 in the UK, targets large enterprises with a focus on secure and scalable solutions, especially in highly regulated industries.\\n   - **Microsoft Power Automate**: Deep integration with Microsoft’s ecosystem makes it appealing for businesses already using Microsoft products.\\n   - **Pega Systems**: Offers extensive BPM capabilities combined with RPA, aiming to streamline both automation and customer engagement processes.\\n   - **NICE**: Focuses on RPA within customer engagement domains, enhances customer service operations.\\n   - **Kofax, WorkFusion, and IBM RPA**: Other significant players with various strengths in comprehensive process automation, AI integration, and cognitive capabilities.\\n\\n3. **Strengths and Strategic Moves by UiPath's Competitors:**\\n   - **Cloud and AI Integration**: Competitors like Automation Anywhere emphasize cloud-native solutions that provide flexibility. AI integration is a common strategic move among competitors.\\n   - **Industry-Specific Solutions**: Companies like Blue Prism and NICE offer tailored solutions for regulated industries, leveraging their strengths in security and compliance.\\n   - **Low-Code/No-Code Platforms**: Players like Pega and Microsoft stand out by enabling non-developer users to harness automation, thus broadening their market appeal.\\n\\n4. **Overall Competitive Analysis:**\\n   - UiPath faces strong competition from both established RPA companies and evolving tech giants like Microsoft.\\n   - Each competitor brings unique features or strategic advantages, such as enhanced integration capabilities, customer-specific solutions, or advanced AI functionalities.\n",
                        "\n",
                        "Report:\n",
                        "# UiPath: Competition Analysis\n",
                        "\n",
                        "## Company Overview\n",
                        "UiPath is a leading player in the Robotic Process Automation (RPA) market, providing software solutions to automate repetitive and mundane tasks across various industries. Founded in 2005, the company has expanded significantly and integrated AI and machine learning technologies into its platform. Headquartered in New York City, UiPath is recognized for its user-friendly, scalable solutions designed for businesses worldwide.\n",
                        "\n",
                        "## Research Objective\n",
                        "To evaluate the competitive landscape of UiPath and understand its position relative to key competitors in the RPA industry.\n",
                        "\n",
                        "## Key Competitors and Their Strategies\n",
                        "\n",
                        "1. **Automation Anywhere**\n",
                        "   - **Founded**: 2003\n",
                        "   - **Headquarter**: San Jose, California\n",
                        "   - **Strengths**: Known for its cloud-native architecture and extensive use of AI. It prioritizes user-friendly automation accessible to a wide range of users.\n",
                        "\n",
                        "2. **Blue Prism**\n",
                        "   - **Founded**: 2001\n",
                        "   - **Headquarter**: London, UK\n",
                        "   - **Strengths**: Focuses on providing secure and scalable automation solutions for large, regulated enterprises. Strong in governance and compliance.\n",
                        "\n",
                        "3. **Microsoft Power Automate**\n",
                        "   - **Founded by Microsoft**\n",
                        "   - **Strengths**: Integrates seamlessly with Microsoft 365 and other products, providing a cost-effective solution for existing Microsoft users.\n",
                        "\n",
                        "4. **Pega Systems**\n",
                        "   - **Founded**: 1983\n",
                        "   - **Headquarter**: Cambridge, Massachusetts\n",
                        "   - **Strengths**: Combines RPA with business process management (BPM) and CRM, offering broader strategic benefits for process automation.\n",
                        "\n",
                        "5. **NICE**\n",
                        "   - **Founded**: 1986\n",
                        "   - **Headquarter**: Ra’anana, Israel\n",
                        "   - **Strengths**: Strong reputation in enhancing customer service operations through RPA, focusing on contact centers.\n",
                        "\n",
                        "6. **Other Notable Competitors**\n",
                        "   - **Kofax**, **WorkFusion**, and **IBM RPA** provide comprehensive process automation with cognitive AI capabilities.\n",
                        "\n",
                        "## Analysis and Insights\n",
                        "- **Cloud Presence and AI Emphasis**: Automation Anywhere and other competitors are investing in cloud and AI to enhance flexibility and intelligence.\n",
                        "- **Security and Compliance**: Blue Prism and NICE cater to highly regulated sectors, providing industry-specific solutions.\n",
                        "- **User Accessibility**: Low-code platforms like Microsoft Power Automate and Pega enable easier adoption by non-developers.\n",
                        "\n",
                        "## Conclusion\n",
                        "UiPath remains a strong leader in the RPA market, but it faces formidable challenges from competitors who are leveraging cloud infrastructure, AI capabilities, and industry-specific solutions to gain market share. Each competitor offers unique strengths, whether through platform integrations, regulatory compliance, or strategic use of AI, positioning them as viable alternatives to UiPath’s offerings.\n"
                    ]
                }
            ],
            "source": [
                "analysis = await research_company(\"UiPath\", \"Competition\")\n",
                "print(analysis)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Try it with your own research topics\n",
                "\n",
                "Now that you've seen how the agent works, you can try researching different companies or objectives. Simply modify the code below with your desired company name and research objective.\n",
                "\n",
                "```python\n",
                "# Example: Research a different company's product strategy\n",
                "# analysis = await research_company(\"Stripe\", \"Product Strategy\")\n",
                "# print(analysis)\n",
                "```\n",
                "\n",
                "Feel free to experiment with different research topics such as:\n",
                "- Company financials\n",
                "- Market positioning\n",
                "- Leadership team\n",
                "- Recent news and developments\n",
                "- Industry trends"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this cookbook, we built a powerful company research agent using Hyperbrowser and OpenAI's GPT-4o. This agent can:\n",
                "\n",
                "1. Automatically search for information about any company\n",
                "2. Intelligently select the most relevant sources\n",
                "3. Extract and process content from those sources\n",
                "4. Generate comprehensive, well-structured research reports\n",
                "5. Save hours of manual research work\n",
                "\n",
                "This pattern can be extended to create more sophisticated research agents for different domains, use additional data sources, or be integrated into larger applications.\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "To take this further, you might consider:\n",
                "- Adding more specialized research objectives (financial analysis, competitive analysis, etc.)\n",
                "- Implementing caching to improve performance for repeated queries\n",
                "- Creating a web interface for easier interaction\n",
                "- Adding support for additional data sources like financial databases or social media\n",
                "- Implementing a feedback loop to improve the quality of reports over time\n",
                "\n",
                "Happy Building! 😃"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Relevant Links\n",
                "- [Hyperbrowser](https://hyperbrowser.ai)\n",
                "- [Hyperbrowser Documentation](https://docs.hyperbrowser.ai)\n",
                "- [OpenAI API Documentation](https://platform.openai.com/docs/introduction)\n",
                "- [Pydantic Documentation](https://docs.pydantic.dev/)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
