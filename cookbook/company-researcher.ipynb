{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Building an AI Company Research Agent with Hyperbrowser and GPT-4o\n",
                "\n",
                "In this cookbook, we'll build a Company Research Agent that can generate detailed reports on any company for any topic by automatically searching the web, analyzing search results, and compiling information from relevant sources.\n",
                "\n",
                "We'll use these tools to build our agent:\n",
                "\n",
                "- **[Hyperbrowser](https://hyperbrowser.ai)** for web search and reading web pages\n",
                "- **OpenAI's GPT-4o** for intelligent analysis and report generation\n",
                "\n",
                "By the end of this cookbook, you'll have a reusable agent that can research any company based on specific objectives, saving hours of manual research work!\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prerequisites\n",
                "\n",
                "To follow along you'll need the following:\n",
                "\n",
                "1. A Hyperbrowser API key (sign up at [hyperbrowser.ai](https://hyperbrowser.ai) if you don't have one, it's free)\n",
                "2. An OpenAI API key (sign up at [openai.com](https://openai.com) if you don't have one, it's free)\n",
                "\n",
                "Both API keys should be stored in a `.env` file in the same directory as this notebook with the following format:\n",
                "\n",
                "```shell\n",
                "HYPERBROWSER_API_KEY=your_hyperbrowser_key_here\n",
                "OPENAI_API_KEY=your_openai_key_here\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Set up imports and load environment variables\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "from urllib.parse import urlencode\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from hyperbrowser import AsyncHyperbrowser\n",
                "from hyperbrowser.models.extract import StartExtractJobParams\n",
                "from hyperbrowser.models.scrape import ScrapeOptions, StartScrapeJobParams\n",
                "from hyperbrowser.models.session import CreateSessionParams\n",
                "from openai import AsyncOpenAI\n",
                "from pydantic import BaseModel\n",
                "\n",
                "\n",
                "import asyncio\n",
                "\n",
                "load_dotenv()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Initialize clients\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "oai = AsyncOpenAI()\n",
                "hb = AsyncHyperbrowser(api_key=os.getenv(\"HYPERBROWSER_API_KEY\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Define search functionality\n",
                "\n",
                "Now we'll create the models and function to search for information about a company. This function:\n",
                "\n",
                "1. Constructs a search URL with the company name and research objective\n",
                "2. Uses Hyperbrowser's extract feature to get structured data from search results\n",
                "3. Returns the search results in a structured format using Pydantic models\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SearchResult(BaseModel):\n",
                "    \"\"\"A search result from Bing\"\"\"\n",
                "\n",
                "    title: str\n",
                "    url: str\n",
                "    content: str\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Title: {self.title}\\nURL: {self.url}\\nContent: {self.content}\"\n",
                "\n",
                "\n",
                "class SearchResults(BaseModel):\n",
                "    \"\"\"A list of search results from Bing\"\"\"\n",
                "\n",
                "    results: list[SearchResult]\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"\\n\\n{'-' * 10}\\n\\n\".join(str(result) for result in self.results)\n",
                "\n",
                "\n",
                "async def search_company(company_name: str, objective: str) -> SearchResults:\n",
                "    params = urlencode({\"q\": f\"{company_name} {objective}\"})\n",
                "    url = f\"https://www.bing.com/search?{params}\"\n",
                "\n",
                "    print(url)\n",
                "\n",
                "    result = await hb.extract.start_and_wait(\n",
                "        StartExtractJobParams(\n",
                "            urls=[url],\n",
                "            prompt=\"Extract the title, url, and content of the top 10 search results on this page.\",\n",
                "            schema=SearchResults,\n",
                "        )\n",
                "    )\n",
                "\n",
                "    if not (result.status == \"completed\" and result.data):\n",
                "        raise Exception(\"Failed to extract search results\")\n",
                "\n",
                "    return SearchResults.model_validate(result.data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Choose relevant URLs with GPT-4o\n",
                "\n",
                "After getting search results, we need to intelligently select the most relevant URLs to scrape. This function:\n",
                "\n",
                "1. Uses GPT-4o to analyze the search results and choose the most relevant URLs\n",
                "2. Applies a system prompt that guides the model to think step-by-step\n",
                "3. Returns both the model's reasoning (chain of thought) and the selected URLs\n",
                "\n",
                "You can also use a different re-ranker model here but for simplicity we've used GPT-4o.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RelevantUrls(BaseModel):\n",
                "    chain_of_thought: str\n",
                "    urls: list[str]\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Chain of Thought: {self.chain_of_thought}\\n\\nURLs:\\n{self.urls}\"\n",
                "\n",
                "\n",
                "RELEVANT_URLS_SYSTEM_PROMPT = \"\"\"\n",
                "You are helping analyze search results to find the most relevant URLs for researching a company. \\\n",
                "Think step by step about which of the URLs you are given would be most useful considering the \\\n",
                "objective and the company name. Always respond with both your chain of thought and the list of \\\n",
                "relevant URLs.\"\"\".strip()\n",
                "\n",
                "\n",
                "async def choose_relevant_urls(\n",
                "    company_name: str, objective: str, search_results: SearchResults\n",
                ") -> RelevantUrls:\n",
                "    response = await oai.beta.chat.completions.parse(\n",
                "        model=\"gpt-4o\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": RELEVANT_URLS_SYSTEM_PROMPT},\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": (\n",
                "                    f\"Company: {company_name}\\nResearch Objective: {objective}\"\n",
                "                    f\"\\n\\nSearch Results:\\n{search_results}\"\n",
                "                    \"Think through which URLs would be most relevant and explain your reasoning. Return your analysis and final list of URLs along with your chain of thought.\"\n",
                "                ),\n",
                "            },\n",
                "        ],\n",
                "        response_format=RelevantUrls,\n",
                "    )\n",
                "\n",
                "    urls = response.choices[0].message.parsed\n",
                "    if urls is None:\n",
                "        raise Exception(\"Could not select relevant urls\")\n",
                "    return urls"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Scrape content from selected URLs\n",
                "\n",
                "Once we have the most relevant URLs, we need to scrape their content. This function:\n",
                "\n",
                "1. Uses Hyperbrowser's batch scraping capability to process multiple URLs efficiently\n",
                "2. Configures advanced scraping options like proxy usage, stealth mode, and CAPTCHA solving\n",
                "3. Returns the scraped content in a structured format for further processing\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from hyperbrowser.models.scrape import ScrapeJobResponse\n",
                "\n",
                "\n",
                "async def scrape_url(url: str):\n",
                "    scrape_result = await hb.scrape.start_and_wait(\n",
                "        StartScrapeJobParams(\n",
                "            url=url,\n",
                "            scrape_options=ScrapeOptions(formats=[\"markdown\"], only_main_content=True),\n",
                "            session_options=CreateSessionParams(\n",
                "                use_proxy=True,\n",
                "                use_stealth=True,\n",
                "                adblock=True,\n",
                "                trackers=True,\n",
                "                annoyances=True,\n",
                "                solve_captchas=True,\n",
                "            ),\n",
                "        )\n",
                "    )\n",
                "    return (url, scrape_result)\n",
                "\n",
                "def limit_content_for_tokens(scrape_results, max_total_chars=20000):\n",
                "    \"\"\"\n",
                "    Intelligently limit scraped content to avoid token limits.\n",
                "    \n",
                "    Args:\n",
                "        scrape_results: List of (url, page) tuples from scraping\n",
                "        max_total_chars: Maximum total characters to allow (default 20000)\n",
                "        \n",
                "    Returns:\n",
                "        String with formatted website content within token limits\n",
                "    \"\"\"\n",
                "    result_texts = []\n",
                "    total_chars = 0\n",
                "    \n",
                "    # First pass: collect all valid results\n",
                "    valid_results = [(url, page.data.markdown) \n",
                "                    for url, page in scrape_results \n",
                "                    if page.data is not None and page.data.markdown is not None]\n",
                "    \n",
                "    if valid_results:\n",
                "        # Calculate chars per site based on available space\n",
                "        chars_per_site = max_total_chars // len(valid_results)\n",
                "        \n",
                "        # Second pass: add truncated content\n",
                "        for url, content in valid_results:\n",
                "            if total_chars >= max_total_chars:\n",
                "                break\n",
                "                \n",
                "            # Truncate if needed\n",
                "            if len(content) > chars_per_site:\n",
                "                content = content[:chars_per_site] + \"...\"\n",
                "            \n",
                "            result_text = f\"<website>\\n<url>{url}</url>\\n<content>\\n{content}\\n</content>\\n</website>\"\n",
                "            result_texts.append(result_text)\n",
                "            total_chars += len(result_text)\n",
                "    \n",
                "    return \"\\n\".join(result_texts)\n",
                "           \n",
                "\n",
                "async def scrape_urls(urls: list[str]) -> str:\n",
                "    try:\n",
                "        scrape_requests = [scrape_url(url) for url in urls]\n",
                "        scrape_results = await asyncio.gather(*scrape_requests)\n",
                "\n",
                "        # If this causes you to run into rate limits, then use this instead\n",
                "        # return limit_content_for_tokens(scrape_results) \n",
                "\n",
                "        return \"\\n\".join(\n",
                "            [\n",
                "                f\"<website>\\n<url>{url}</url>\\n<content>\\n{page.data.markdown}\\n</content>\\n</website>\"\n",
                "                for url, page in scrape_results\n",
                "                if page.data is not None and page.data.markdown is not None\n",
                "            ]\n",
                "        )\n",
                "       \n",
                "        \n",
                "    except Exception as e:\n",
                "        raise Exception(\"Failed to scrape URLS\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Generate research report with GPT-4o\n",
                "\n",
                "With the scraped content in hand, we can now generate a comprehensive research report. This function:\n",
                "\n",
                "1. Uses GPT-4o with a specialized system prompt for research analysis\n",
                "2. Provides the model with the company name, research objective, and scraped data\n",
                "3. Returns both the model's reasoning process and the final formatted report\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ResearchAnalysis(BaseModel):\n",
                "    chain_of_thought: str\n",
                "    report: str\n",
                "\n",
                "    def __str__(self):\n",
                "        return f\"Chain of Thought:\\n{self.chain_of_thought}\\n\\nReport:\\n{self.report}\"\n",
                "\n",
                "\n",
                "FINAL_REPORT_SYSTEM_PROMPT = \"\"\"\n",
                "You are an expert research analyst that compiles information from a list of websites into a report about a company.\n",
                "\n",
                "You will be given the following information:\n",
                "- Company Name\n",
                "- Research Objective\n",
                "- Scraped Data from the most relevant webpages about the company as it relates to the research objective / topic\n",
                "\n",
                "Your job is to compile the information into a report that is easy to understand and can be used to make decisions about the company.\n",
                "\n",
                "The report should be:\n",
                "- In markdown format\n",
                "- Concise and to the point\n",
                "- Include the company name, objective, and the information found on the websites\n",
                "- Be easy to understand and can be used to make decisions about the company\n",
                "\n",
                "Additionally, you should also maintain a scratchpad for your own notes and thoughts about the company as you draft the report.\n",
                "\n",
                "You must respond with both your chain of thought and the final report.\"\"\".strip()\n",
                "\n",
                "\n",
                "async def compile_into_report(\n",
                "    scraped_data: str, company_name: str, objective: str\n",
                ") -> ResearchAnalysis:\n",
                "    response = await oai.beta.chat.completions.parse(\n",
                "        model=\"gpt-4o\",\n",
                "        messages=[\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": FINAL_REPORT_SYSTEM_PROMPT,\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": (\n",
                "                    f\"Company Name: {company_name}\\nResearch Objective: {objective}\"\n",
                "                    f\"\\nScraped Websites / Articles: {scraped_data}\"\n",
                "                ),\n",
                "            },\n",
                "        ],\n",
                "        response_format=ResearchAnalysis,\n",
                "    )\n",
                "\n",
                "    analysis = response.choices[0].message.parsed\n",
                "    if (analysis is None):\n",
                "        raise Exception(f\"Could not get analysis for {company_name}\")\n",
                "\n",
                "    return analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Combine all steps into a single research pipeline\n",
                "\n",
                "Now we'll create a single function that orchestrates the entire research process from start to finish. This function:\n",
                "\n",
                "1. Searches for information about the company\n",
                "2. Selects the most relevant URLs\n",
                "3. Scrapes content from those URLs\n",
                "4. Generates a comprehensive research report\n",
                "5. Returns the final analysis\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def research_company(company_name: str, objective: str) -> ResearchAnalysis:\n",
                "    search_results = await search_company(company_name, objective)\n",
                "    relevant_urls = await choose_relevant_urls(company_name, objective, search_results)\n",
                "    scraped_data = await scrape_urls(relevant_urls.urls)\n",
                "    analysis = await compile_into_report(scraped_data, company_name, objective)\n",
                "    return analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Test the research agent\n",
                "\n",
                "Let's test our agent by researching UiPath's competitive landscape. This will demonstrate the full workflow:\n",
                "\n",
                "1. The agent searches for information about UiPath's competition\n",
                "2. It selects the most relevant URLs from the search results\n",
                "3. It scrapes content from those URLs\n",
                "4. It generates a comprehensive research report\n",
                "\n",
                "You'll see the search URL being printed, followed by the final analysis that includes both the reasoning process and the formatted report.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "https://www.bing.com/search?q=UiPath+Competition\n",
                        "Chain of Thought:\n",
                        "With UiPath as one of the leading names in the robotic process automation (RPA) industry, it faces significant competition from various competitors that offer both alternative and complementary technology solutions. Companies like Automation Anywhere, Blue Prism, and Microsoft Power Automate are direct RPA competitors, each offering unique benefits that challenge UiPath's dominance. Zluri and WorkFusion also provide distinctive capabilities like cloud-native platforms and AI integration, respectively. Other industry entrants, such as IBM, Pega, and NICE, present robust RPA solutions tailored to specific industry needs, highlighting important comparative elements like ease of integration, scalability, and pricing. Evaluating UiPath against key competitors reveals both strategic opportunities and challenges due to the rapidly evolving landscape of enterprise automation technologies.\n",
                        "\n",
                        "Report:\n",
                        "# Company Report: UiPath\n",
                        "\n",
                        "## **Research Objective:**\n",
                        "Assessing the competitive landscape of UiPath.\n",
                        "\n",
                        "## **Overview**\n",
                        "UiPath is a leader in robotic process automation (RPA), known for its robust suite of automation tools that incorporate AI and ML to optimize complex processes. Despite its leadership position, UiPath competes with several formidable platforms that vie for a share in the growing automation market.\n",
                        "\n",
                        "## **Major Competitors**\n",
                        "\n",
                        "### 1. **Automation Anywhere**\n",
                        "* **Founded:** 2003\n",
                        "* **Headquarters:** San Jose, California\n",
                        "* **Strengths:** Leading cloud-native RPA platform, extensive tools for both attended and unattended automation, strong presence with its Bot Store for pre-built automation tools.\n",
                        "* **Notes:** Competes on ease of use with its user-friendly interface and lower deployment cost for smaller enterprises.\n",
                        "\n",
                        "### 2. **Blue Prism**\n",
                        "* **Founded:** 2001\n",
                        "* **Headquarters:** Warrington, UK\n",
                        "* **Strengths:** Strong focus on secure, enterprise-grade solutions with robust audit capabilities, ideal for regulated industries.\n",
                        "* **Notes:** Known for seamless integration with older systems and strong governance tools.\n",
                        "\n",
                        "### 3. **Microsoft Power Automate**\n",
                        "* **Founded:** 2016\n",
                        "* **Headquarters:** Redmond, Washington\n",
                        "* **Strengths:** Deep integration with Microsoft's ecosystem, user-friendly low-code solutions.\n",
                        "* **Notes:** Attractive for organizations deeply invested in Microsoft products.\n",
                        "\n",
                        "### 4. **WorkFusion**\n",
                        "* **Founded:** 2010\n",
                        "* **Headquarters:** New York City, New York\n",
                        "* **Strengths:** AI-powered tools for automating document parsing and complex data interactions, leveraging NLP and ML.\n",
                        "* **Notes:** Particularly strong in verticals like insurance and finance.\n",
                        "\n",
                        "### 5. **Pega**\n",
                        "* **Founded:** 1983\n",
                        "* **Headquarters:** Cambridge, Massachusetts\n",
                        "* **Strengths:** Comprehensive automation platform that includes BPM, RPA, and CRM solutions.\n",
                        "* **Notes:** Serves complex organizational processes beyond task automation.\n",
                        "\n",
                        "### 6. **IBM RPA**\n",
                        "* **Founded:** 1911\n",
                        "* **Headquarters:** Armonk, New York\n",
                        "* **Strengths:** Combines automation with cognitive capabilities and robust analytics.\n",
                        "* **Notes:** Targets large enterprises with a comprehensive, integrated suite.\n",
                        "\n",
                        "## **Competitive Advantages of UiPath**\n",
                        "* **User-Friendly Interface:** Known for its intuitive design, making automation accessible to non-technical users.\n",
                        "* **Scalability:** Suitable for both small-scale operations and large enterprise deployments.\n",
                        "* **Community and Ecosystem:** Large user and developer community, contributing to continuous innovation and support.\n",
                        "* **Integration Capabilities:** Extensive integrations with various cloud platforms and business tools.\n",
                        "\n",
                        "## **Decision Factors**\n",
                        "When choosing between UiPath and its competitors, organizations should consider:\n",
                        "- **Integration needs**—alignment with existing software solutions.\n",
                        "- **Industry-specific requirements**—such as compliance and security regulations.\n",
                        "- **Cost efficiency**—considering not only initial setup costs but also ongoing expenses and potential ROI.\n",
                        "- **Ease of deployment and user adoption.** \n",
                        "\n",
                        "Overall, UiPath maintains a competitive edge due to its strong brand reputation, versatile platform capabilities, and supportive community, though constant innovation remains necessary to stay ahead in the rapidly evolving RPA field.\n"
                    ]
                }
            ],
            "source": [
                "analysis = await research_company(\"UiPath\", \"Competition\")\n",
                "print(analysis)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Try it with your own research topics\n",
                "\n",
                "Now that you've seen how the agent works, you can try researching different companies or objectives. Simply modify the code below with your desired company name and research objective.\n",
                "\n",
                "```python\n",
                "# Example: Research a different company's product strategy\n",
                "# analysis = await research_company(\"Stripe\", \"Product Strategy\")\n",
                "# print(analysis)\n",
                "```\n",
                "\n",
                "Feel free to experiment with different research topics such as:\n",
                "\n",
                "- Company financials\n",
                "- Market positioning\n",
                "- Leadership team\n",
                "- Recent news and developments\n",
                "- Industry trends\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "In this cookbook, we built a powerful company research agent using Hyperbrowser and OpenAI's GPT-4o. This agent can:\n",
                "\n",
                "1. Automatically search for information about any company\n",
                "2. Intelligently select the most relevant sources\n",
                "3. Extract and process content from those sources\n",
                "4. Generate comprehensive, well-structured research reports\n",
                "5. Save hours of manual research work\n",
                "\n",
                "This pattern can be extended to create more sophisticated research agents for different domains, use additional data sources, or be integrated into larger applications.\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "To take this further, you might consider:\n",
                "\n",
                "- Adding more specialized research objectives (financial analysis, competitive analysis, etc.)\n",
                "- Implementing caching to improve performance for repeated queries\n",
                "- Creating a web interface for easier interaction\n",
                "- Adding support for additional data sources like financial databases or social media\n",
                "- Implementing a feedback loop to improve the quality of reports over time\n",
                "\n",
                "Happy Building! 😃\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Relevant Links\n",
                "\n",
                "- [Hyperbrowser](https://hyperbrowser.ai)\n",
                "- [Hyperbrowser Documentation](https://docs.hyperbrowser.ai)\n",
                "- [OpenAI API Documentation](https://platform.openai.com/docs/introduction)\n",
                "- [Pydantic Documentation](https://docs.pydantic.dev/)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
